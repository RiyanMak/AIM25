{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0\n",
      "MPS available: True\n",
      "MPS built: True\n",
      "Using MPS device: mps\n",
      "Filtering dataset: /path/to/your/data/pd_annotations_balanced.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744855914.330521 1984146 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M2\n",
      "W0000 00:00:1744855914.331939 1996382 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1744855914.335970 1996385 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/path/to/your/data/pd_annotations_balanced.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/riyan/Desktop/AIM Spring 2025/PDSecondApproach.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m   <a href='vscode-notebook-cell:/Users/riyan/Desktop/AIM%20Spring%202025/PDSecondApproach.ipynb#W0sZmlsZQ%3D%3D?line=1186'>1187</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m run_live_detection\n\u001b[1;32m   <a href='vscode-notebook-cell:/Users/riyan/Desktop/AIM%20Spring%202025/PDSecondApproach.ipynb#W0sZmlsZQ%3D%3D?line=1188'>1189</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> <a href='vscode-notebook-cell:/Users/riyan/Desktop/AIM%20Spring%202025/PDSecondApproach.ipynb#W0sZmlsZQ%3D%3D?line=1189'>1190</a>\u001b[0m     run_live_detection \u001b[39m=\u001b[39m main()\n\u001b[1;32m   <a href='vscode-notebook-cell:/Users/riyan/Desktop/AIM%20Spring%202025/PDSecondApproach.ipynb#W0sZmlsZQ%3D%3D?line=1190'>1191</a>\u001b[0m     \u001b[39m# Uncomment to run live detection immediately\u001b[39;00m\n\u001b[1;32m   <a href='vscode-notebook-cell:/Users/riyan/Desktop/AIM%20Spring%202025/PDSecondApproach.ipynb#W0sZmlsZQ%3D%3D?line=1191'>1192</a>\u001b[0m     run_live_detection()\n",
      "\u001b[1;32m/Users/riyan/Desktop/AIM Spring 2025/PDSecondApproach.ipynb Cell 1\u001b[0m line \u001b[0;36m9\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/riyan/Desktop/AIM%20Spring%202025/PDSecondApproach.ipynb#W0sZmlsZQ%3D%3D?line=907'>908</a>\u001b[0m emotion_model_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/path/to/your/emotion_model.pth\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/riyan/Desktop/AIM%20Spring%202025/PDSecondApproach.ipynb#W0sZmlsZQ%3D%3D?line=909'>910</a>\u001b[0m \u001b[39m# Filter original dataset to only include images with detectable faces\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/riyan/Desktop/AIM%20Spring%202025/PDSecondApproach.ipynb#W0sZmlsZQ%3D%3D?line=910'>911</a>\u001b[0m \u001b[39m# This is crucial for preventing errors later\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/riyan/Desktop/AIM%20Spring%202025/PDSecondApproach.ipynb#W0sZmlsZQ%3D%3D?line=911'>912</a>\u001b[0m filter_dataset(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/riyan/Desktop/AIM%20Spring%202025/PDSecondApproach.ipynb#W0sZmlsZQ%3D%3D?line=912'>913</a>\u001b[0m     root_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/riyan/Desktop/AIM%20Spring%202025/PDSecondApproach.ipynb#W0sZmlsZQ%3D%3D?line=913'>914</a>\u001b[0m     annotations_file\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(data_dir, \u001b[39m'\u001b[39;49m\u001b[39mpd_annotations_balanced.csv\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/riyan/Desktop/AIM%20Spring%202025/PDSecondApproach.ipynb#W0sZmlsZQ%3D%3D?line=914'>915</a>\u001b[0m     output_file\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(data_dir, \u001b[39m'\u001b[39;49m\u001b[39mpd_annotations_filtered.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/riyan/Desktop/AIM%20Spring%202025/PDSecondApproach.ipynb#W0sZmlsZQ%3D%3D?line=915'>916</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/riyan/Desktop/AIM%20Spring%202025/PDSecondApproach.ipynb#W0sZmlsZQ%3D%3D?line=917'>918</a>\u001b[0m \u001b[39m# Split into train/val sets (80/20 split)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/riyan/Desktop/AIM%20Spring%202025/PDSecondApproach.ipynb#W0sZmlsZQ%3D%3D?line=918'>919</a>\u001b[0m annotations \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_dir, \u001b[39m'\u001b[39m\u001b[39mpd_annotations_filtered.csv\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[1;32m/Users/riyan/Desktop/AIM Spring 2025/PDSecondApproach.ipynb Cell 1\u001b[0m line \u001b[0;36m4\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/riyan/Desktop/AIM%20Spring%202025/PDSecondApproach.ipynb#W0sZmlsZQ%3D%3D?line=474'>475</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Create a new dataset with only images where face detection succeeds\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/riyan/Desktop/AIM%20Spring%202025/PDSecondApproach.ipynb#W0sZmlsZQ%3D%3D?line=475'>476</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFiltering dataset: \u001b[39m\u001b[39m{\u001b[39;00mannotations_file\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/riyan/Desktop/AIM%20Spring%202025/PDSecondApproach.ipynb#W0sZmlsZQ%3D%3D?line=477'>478</a>\u001b[0m annotations \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(annotations_file)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/riyan/Desktop/AIM%20Spring%202025/PDSecondApproach.ipynb#W0sZmlsZQ%3D%3D?line=478'>479</a>\u001b[0m valid_rows \u001b[39m=\u001b[39m []\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/riyan/Desktop/AIM%20Spring%202025/PDSecondApproach.ipynb#W0sZmlsZQ%3D%3D?line=479'>480</a>\u001b[0m au_extractor \u001b[39m=\u001b[39m FacialActionUnitExtractor()\n",
      "File \u001b[0;32m~/Desktop/AIM Spring 2025/.venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/Desktop/AIM Spring 2025/.venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Desktop/AIM Spring 2025/.venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/Desktop/AIM Spring 2025/.venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Desktop/AIM Spring 2025/.venv/lib/python3.9/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/your/data/pd_annotations_balanced.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pickle\n",
    "\n",
    "# ===== SETUP =====\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "print(f\"MPS built: {torch.backends.mps.is_built()}\")\n",
    "\n",
    "# Set device (MPS for M1/M2, CUDA for NVIDIA, CPU otherwise)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"Using MPS device: {device}\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using CUDA device: {device}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Performance optimization\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Helper function to free memory\n",
    "def empty_cache():\n",
    "    \"\"\"Empty GPU cache to free memory\"\"\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "    elif torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# ===== FACIAL LANDMARK EXTRACTION =====\n",
    "\n",
    "# Initialize MediaPipe Face Mesh with more forgiving parameters\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=True,\n",
    "    max_num_faces=1,\n",
    "    min_detection_confidence=0.3)  # Lowered from 0.5 to improve detection rate\n",
    "\n",
    "# Add logging function\n",
    "def log_detection_failure(image_path=None):\n",
    "    \"\"\"Log face detection failures\"\"\"\n",
    "    if image_path:\n",
    "        print(f\"Face detection failed for image: {image_path}\")\n",
    "    else:\n",
    "        print(\"Face detection failed\")\n",
    "\n",
    "class FacialActionUnitExtractor:\n",
    "    def __init__(self):\n",
    "        # Key AU landmarks for PD detection based on the paper\n",
    "        self.au_landmarks = {\n",
    "            'AU1': [10, 338],   # Inner brow raiser\n",
    "            'AU2': [65, 295],   # Outer brow raiser\n",
    "            'AU4': [9, 337],    # Brow lowerer - key for PD\n",
    "            'AU6': [117, 346],  # Cheek raiser - key for PD\n",
    "            'AU7': [159, 386],  # Lid tightener\n",
    "            'AU9': [129, 358],  # Nose wrinkler\n",
    "            'AU12': [61, 291],  # Lip corner puller (smile) - key for PD\n",
    "            'AU15': [61, 291],  # Lip corner depressor\n",
    "            'AU20': [0, 267]    # Lip stretcher\n",
    "        }\n",
    "        \n",
    "        # PD-specific facial measurements\n",
    "        self.pd_measurements = [\n",
    "            'smile_symmetry',       # Asymmetry in smile - PD indicator\n",
    "            'blink_rate',           # Reduced blink rate - PD indicator\n",
    "            'facial_mobility',      # Reduced overall mobility - PD indicator\n",
    "            'expression_transition' # Slow transitions - PD indicator\n",
    "        ]\n",
    "    \n",
    "    def extract_aus(self, image, image_path=None):\n",
    "        \"\"\"Extract facial action units with focus on PD-relevant features\"\"\"\n",
    "        # Convert image to RGB for MediaPipe\n",
    "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        h, w = rgb_image.shape[:2]  # Image dimensions for correct scaling\n",
    "        \n",
    "        # Process the image to find facial landmarks\n",
    "        results = face_mesh.process(rgb_image)\n",
    "        \n",
    "        # Initialize AU values - 9 core AUs + 4 PD-specific measurements\n",
    "        aus = np.zeros(13)\n",
    "        \n",
    "        # Check if face was detected\n",
    "        if not results.multi_face_landmarks:\n",
    "            if image_path:\n",
    "                log_detection_failure(image_path)\n",
    "            return aus, False  # Return zeros and False for detection failure\n",
    "        \n",
    "        landmarks = results.multi_face_landmarks[0].landmark\n",
    "        \n",
    "        try:\n",
    "            # Convert landmarks to numpy array with correct scaling\n",
    "            points = np.array([(lm.x * w, lm.y * h, lm.z) for lm in landmarks])\n",
    "            \n",
    "            # Extract key PD-related AUs\n",
    "            # 1. AU6 (Cheek Raiser) - key for PD detection\n",
    "            cheek_raise_left = np.linalg.norm(points[117] - points[123])\n",
    "            cheek_raise_right = np.linalg.norm(points[346] - points[352])\n",
    "            aus[0] = (cheek_raise_left + cheek_raise_right) / 2\n",
    "            \n",
    "            # 2. AU12 (Lip Corner Puller) - key for PD detection\n",
    "            mouth_left = points[61]\n",
    "            mouth_right = points[291]\n",
    "            mouth_top = points[13]\n",
    "            mouth_bottom = points[14]\n",
    "            mouth_center = (mouth_top + mouth_bottom) / 2\n",
    "            smile_measure = mouth_center[1] - (mouth_left[1] + mouth_right[1])/2\n",
    "            aus[1] = smile_measure\n",
    "            \n",
    "            # 3. AU4 (Brow Lowerer) - key for PD detection\n",
    "            brow_lower_left = np.linalg.norm(points[9] - points[107])\n",
    "            brow_lower_right = np.linalg.norm(points[337] - points[336])\n",
    "            aus[2] = (brow_lower_left + brow_lower_right) / 2\n",
    "            \n",
    "            # 4. AU1 (Inner Brow Raiser)\n",
    "            inner_brow_raise = np.linalg.norm(points[10] - points[338])\n",
    "            aus[3] = inner_brow_raise\n",
    "            \n",
    "            # 5. AU2 (Outer Brow Raiser)\n",
    "            outer_brow_raise = np.linalg.norm(points[65] - points[295])\n",
    "            aus[4] = outer_brow_raise\n",
    "            \n",
    "            # 6. AU7 (Lid Tightener)\n",
    "            lid_tighten_left = np.linalg.norm(points[159] - points[145])\n",
    "            lid_tighten_right = np.linalg.norm(points[386] - points[374])\n",
    "            aus[5] = (lid_tighten_left + lid_tighten_right) / 2\n",
    "            \n",
    "            # 7. AU9 (Nose Wrinkler)\n",
    "            nose_wrinkle = np.linalg.norm(points[129] - points[358])\n",
    "            aus[6] = nose_wrinkle\n",
    "            \n",
    "            # 8. AU15 (Lip Corner Depressor)\n",
    "            lip_corner_depress = np.linalg.norm(points[61] - points[291])\n",
    "            aus[7] = lip_corner_depress\n",
    "            \n",
    "            # 9. AU20 (Lip Stretcher)\n",
    "            lip_stretch = np.linalg.norm(points[0] - points[267])\n",
    "            aus[8] = lip_stretch\n",
    "            \n",
    "            # PD-specific measurements\n",
    "            # 10. Smile symmetry - PD often has asymmetrical facial expressions\n",
    "            left_smile = np.linalg.norm(mouth_left - mouth_top)\n",
    "            right_smile = np.linalg.norm(mouth_right - mouth_top)\n",
    "            # Add epsilon to prevent division by zero\n",
    "            epsilon = 1e-6\n",
    "            smile_asymmetry = abs(left_smile - right_smile) / (max(left_smile, right_smile) + epsilon)\n",
    "            aus[9] = smile_asymmetry\n",
    "            \n",
    "            # 11. Blink rate approximation (eye openness)\n",
    "            left_eye_open = np.linalg.norm(points[159] - points[145])\n",
    "            right_eye_open = np.linalg.norm(points[386] - points[374])\n",
    "            eye_openness = (left_eye_open + right_eye_open) / 2\n",
    "            aus[10] = eye_openness\n",
    "            \n",
    "            # 12. Overall facial mobility (average movement potential)\n",
    "            # Use max to prevent negative values from smile_measure which might be negative\n",
    "            facial_mobility = (max(0, aus[0]) + max(0, abs(aus[1])) + max(0, aus[2])) / 3\n",
    "            aus[11] = facial_mobility\n",
    "            \n",
    "            # 13. Mouth corner resting position (hypomimia indicator)\n",
    "            mouth_corner_rest = (mouth_left[1] + mouth_right[1]) / 2\n",
    "            aus[12] = mouth_corner_rest\n",
    "            \n",
    "            return aus, True  # Return AUs and True for successful detection\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting AUs: {e}\")\n",
    "            if image_path:\n",
    "                log_detection_failure(image_path)\n",
    "            return aus, False  # Return zeros and False for extraction failure\n",
    "\n",
    "# ===== DATASET PREPARATION =====\n",
    "\n",
    "class PDDataset(Dataset):\n",
    "    def __init__(self, root_dir, annotations_file, transform=None, seq_length=30, target_size=(224, 224),\n",
    "                inspect_features=False, balance_classes=True):\n",
    "        \"\"\"\n",
    "        Dataset for Parkinson's Disease detection\n",
    "        \n",
    "        Args:\n",
    "            root_dir: Directory with all the images/videos\n",
    "            annotations_file: Path to CSV file with annotations\n",
    "            transform: Optional transform to be applied\n",
    "            seq_length: Number of frames to extract\n",
    "            target_size: Target size for frames\n",
    "            inspect_features: Whether to save feature statistics for inspection\n",
    "            balance_classes: Whether to balance classes in the dataset\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.seq_length = seq_length\n",
    "        self.target_size = target_size\n",
    "        self.au_extractor = FacialActionUnitExtractor()\n",
    "        self.inspect_features = inspect_features\n",
    "        self.face_detection_failures = 0\n",
    "        self.total_processed = 0\n",
    "        \n",
    "        # Read annotations\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        \n",
    "        # Balance classes if requested\n",
    "        if balance_classes:\n",
    "            pd_samples = self.annotations[self.annotations['has_pd'] == 1]\n",
    "            non_pd_samples = self.annotations[self.annotations['has_pd'] == 0]\n",
    "            \n",
    "            # Downsample majority class or upsample minority class\n",
    "            if len(pd_samples) < len(non_pd_samples):\n",
    "                # Downsample non-PD\n",
    "                non_pd_samples = non_pd_samples.sample(n=len(pd_samples), random_state=42)\n",
    "                self.annotations = pd.concat([pd_samples, non_pd_samples])\n",
    "            elif len(pd_samples) > len(non_pd_samples):\n",
    "                # Upsample PD (with replacement)\n",
    "                pd_samples = pd_samples.sample(n=len(non_pd_samples), random_state=42, replace=True)\n",
    "                self.annotations = pd.concat([pd_samples, non_pd_samples])\n",
    "        \n",
    "        # Shuffle\n",
    "        self.annotations = self.annotations.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "        # Report class distribution\n",
    "        pd_count = sum(self.annotations['has_pd'])\n",
    "        total = len(self.annotations)\n",
    "        print(f\"Dataset loaded: {total} samples\")\n",
    "        print(f\"Class distribution: PD={pd_count} ({pd_count/total*100:.2f}%), Non-PD={total-pd_count} ({(total-pd_count)/total*100:.2f}%)\")\n",
    "        \n",
    "        # For feature inspection\n",
    "        if inspect_features:\n",
    "            self.pd_features = []\n",
    "            self.non_pd_features = []\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get label (PD or non-PD)\n",
    "        has_pd = self.annotations.iloc[idx].get('has_pd', 0)\n",
    "        \n",
    "        # Get filename - handle both image and video datasets\n",
    "        if 'video_file' in self.annotations.columns:\n",
    "            file_path = os.path.join(self.root_dir, self.annotations.iloc[idx]['video_file'])\n",
    "            is_video = True\n",
    "        else:\n",
    "            file_path = os.path.join(self.root_dir, self.annotations.iloc[idx]['image_file'])\n",
    "            is_video = False\n",
    "        \n",
    "        # Extract features\n",
    "        if is_video:\n",
    "            # Process video file\n",
    "            sequence, variances, detection_succeeded = self._process_video(file_path)\n",
    "        else:\n",
    "            # Process image file (with simulated sequence)\n",
    "            sequence, variances, detection_succeeded = self._process_image(file_path)\n",
    "        \n",
    "        # Track detection failures\n",
    "        self.total_processed += 1\n",
    "        if not detection_succeeded:\n",
    "            self.face_detection_failures += 1\n",
    "            if self.total_processed % 100 == 0:\n",
    "                print(f\"Face detection failures: {self.face_detection_failures}/{self.total_processed} ({self.face_detection_failures/self.total_processed*100:.2f}%)\")\n",
    "        \n",
    "        # Store features for inspection if needed and if detection succeeded\n",
    "        if self.inspect_features and detection_succeeded:\n",
    "            if has_pd == 1:\n",
    "                self.pd_features.append(variances.numpy())\n",
    "            else:\n",
    "                self.non_pd_features.append(variances.numpy())\n",
    "        \n",
    "        # Return tensors\n",
    "        label_tensor = torch.FloatTensor([has_pd])\n",
    "        \n",
    "        return sequence, variances, label_tensor\n",
    "    \n",
    "    def _process_video(self, video_path):\n",
    "        \"\"\"Process a video file to extract AU sequence and variances\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Could not open video file: {video_path}\")\n",
    "            # Return default values for failure\n",
    "            default_sequence = torch.zeros((self.seq_length, 13))\n",
    "            default_variances = torch.zeros(13)\n",
    "            return default_sequence, default_variances, False\n",
    "        \n",
    "        # Get frame count\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        # Select frames\n",
    "        if frame_count <= self.seq_length:\n",
    "            indices = np.linspace(0, frame_count-1, self.seq_length, dtype=int)\n",
    "        else:\n",
    "            indices = np.linspace(0, frame_count-1, self.seq_length, dtype=int)\n",
    "        \n",
    "        # Extract AUs\n",
    "        au_sequence = []\n",
    "        detection_count = 0\n",
    "        \n",
    "        for frame_idx in indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame = cv2.resize(frame, self.target_size)\n",
    "                aus, detected = self.au_extractor.extract_aus(frame, video_path)\n",
    "                au_sequence.append(aus)\n",
    "                if detected:\n",
    "                    detection_count += 1\n",
    "            else:\n",
    "                if au_sequence:\n",
    "                    au_sequence.append(au_sequence[-1])\n",
    "                else:\n",
    "                    au_sequence.append(np.zeros(13))\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        # Calculate detection success rate\n",
    "        detection_success_rate = detection_count / self.seq_length\n",
    "        detection_succeeded = detection_success_rate > 0.5  # Require at least 50% frames with detected faces\n",
    "        \n",
    "        # Calculate variances (key feature according to the paper)\n",
    "        # Add small epsilon to prevent NaN values\n",
    "        epsilon = 1e-6\n",
    "        au_variances = np.var(au_sequence, axis=0) + epsilon\n",
    "        \n",
    "        # Convert to tensors\n",
    "        sequence_np = np.array(au_sequence)\n",
    "        au_sequence_tensor = torch.FloatTensor(sequence_np)\n",
    "        au_variances_tensor = torch.FloatTensor(au_variances)\n",
    "        \n",
    "        return au_sequence_tensor, au_variances_tensor, detection_succeeded\n",
    "    \n",
    "    def _process_image(self, image_path):\n",
    "        \"\"\"Process an image file with simulated sequence for temporal features\"\"\"\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            print(f\"Could not find image: {image_path}\")\n",
    "            # Return default values for failure\n",
    "            default_sequence = torch.zeros((self.seq_length, 13))\n",
    "            default_variances = torch.zeros(13)\n",
    "            return default_sequence, default_variances, False\n",
    "        \n",
    "        img = cv2.resize(img, self.target_size)\n",
    "        aus, detected = self.au_extractor.extract_aus(img, image_path)\n",
    "        \n",
    "        if not detected:\n",
    "            # Return default values for failure\n",
    "            default_sequence = torch.zeros((self.seq_length, 13))\n",
    "            default_variances = torch.zeros(13)\n",
    "            return default_sequence, default_variances, False\n",
    "        \n",
    "        # Generate a sequence with subtle variations to simulate a video\n",
    "        au_sequence = []\n",
    "        for i in range(self.seq_length):\n",
    "            # Create variations that decrease as frames progress (simulating expression change)\n",
    "            decay_factor = (self.seq_length - i) / self.seq_length\n",
    "            variation = np.random.normal(0, 0.02 * decay_factor, size=aus.shape)\n",
    "            au_sequence.append(aus + variation)\n",
    "        \n",
    "        # Calculate variances with small epsilon to prevent NaN\n",
    "        epsilon = 1e-6\n",
    "        au_variances = np.var(au_sequence, axis=0) + epsilon\n",
    "        \n",
    "        # Convert to tensors\n",
    "        sequence_np = np.array(au_sequence)\n",
    "        au_sequence_tensor = torch.FloatTensor(sequence_np)\n",
    "        au_variances_tensor = torch.FloatTensor(au_variances)\n",
    "        \n",
    "        return au_sequence_tensor, au_variances_tensor, True\n",
    "    \n",
    "    def get_feature_statistics(self):\n",
    "        \"\"\"Get statistics of features for PD and non-PD samples\"\"\"\n",
    "        if not self.inspect_features:\n",
    "            raise ValueError(\"Feature inspection was not enabled for this dataset\")\n",
    "        \n",
    "        # Handle case with no valid features\n",
    "        if not self.pd_features or not self.non_pd_features:\n",
    "            print(\"Warning: No valid features collected. Face detection likely failed for all images.\")\n",
    "            \n",
    "            # Return default values\n",
    "            feature_names = [\n",
    "                \"AU6 (Cheek Raiser)\", \n",
    "                \"AU12 (Lip Corner Puller)\",\n",
    "                \"AU4 (Brow Lowerer)\",\n",
    "                \"AU1 (Inner Brow Raiser)\",\n",
    "                \"AU2 (Outer Brow Raiser)\",\n",
    "                \"AU7 (Lid Tightener)\",\n",
    "                \"AU9 (Nose Wrinkler)\",\n",
    "                \"AU15 (Lip Corner Depressor)\",\n",
    "                \"AU20 (Lip Stretcher)\",\n",
    "                \"Smile Asymmetry\",\n",
    "                \"Eye Openness\",\n",
    "                \"Facial Mobility\",\n",
    "                \"Mouth Corner Rest\"\n",
    "            ]\n",
    "            \n",
    "            # Create placeholder statistics\n",
    "            pd_means = np.zeros(13)\n",
    "            non_pd_means = np.zeros(13)\n",
    "            pd_std = np.zeros(13)\n",
    "            non_pd_std = np.zeros(13)\n",
    "            \n",
    "            stats = {\n",
    "                \"feature_names\": feature_names,\n",
    "                \"pd_means\": pd_means,\n",
    "                \"non_pd_means\": non_pd_means,\n",
    "                \"pd_std\": pd_std,\n",
    "                \"non_pd_std\": non_pd_std,\n",
    "                \"differences\": pd_means - non_pd_means,\n",
    "                \"t_statistics\": np.zeros(13)\n",
    "            }\n",
    "            \n",
    "            return stats\n",
    "        \n",
    "        pd_features = np.array(self.pd_features)\n",
    "        non_pd_features = np.array(self.non_pd_features)\n",
    "        \n",
    "        pd_means = np.mean(pd_features, axis=0)\n",
    "        non_pd_means = np.mean(non_pd_features, axis=0)\n",
    "        \n",
    "        pd_std = np.std(pd_features, axis=0)\n",
    "        non_pd_std = np.std(non_pd_features, axis=0)\n",
    "        \n",
    "        # Add epsilon to prevent division by zero\n",
    "        epsilon = 1e-6\n",
    "        t_statistics = (pd_means - non_pd_means) / np.sqrt(\n",
    "            pd_std**2/(len(pd_features) + epsilon) + \n",
    "            non_pd_std**2/(len(non_pd_features) + epsilon) + \n",
    "            epsilon\n",
    "        )\n",
    "        \n",
    "        feature_names = [\n",
    "            \"AU6 (Cheek Raiser)\", \n",
    "            \"AU12 (Lip Corner Puller)\",\n",
    "            \"AU4 (Brow Lowerer)\",\n",
    "            \"AU1 (Inner Brow Raiser)\",\n",
    "            \"AU2 (Outer Brow Raiser)\",\n",
    "            \"AU7 (Lid Tightener)\",\n",
    "            \"AU9 (Nose Wrinkler)\",\n",
    "            \"AU15 (Lip Corner Depressor)\",\n",
    "            \"AU20 (Lip Stretcher)\",\n",
    "            \"Smile Asymmetry\",\n",
    "            \"Eye Openness\",\n",
    "            \"Facial Mobility\",\n",
    "            \"Mouth Corner Rest\"\n",
    "        ]\n",
    "        \n",
    "        stats = {\n",
    "            \"feature_names\": feature_names,\n",
    "            \"pd_means\": pd_means,\n",
    "            \"non_pd_means\": non_pd_means,\n",
    "            \"pd_std\": pd_std,\n",
    "            \"non_pd_std\": non_pd_std,\n",
    "            \"differences\": pd_means - non_pd_means,\n",
    "            \"t_statistics\": t_statistics\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# ===== NEW FILTERING FUNCTION =====\n",
    "\n",
    "def filter_dataset(root_dir, annotations_file, output_file):\n",
    "    \"\"\"Create a new dataset with only images where face detection succeeds\"\"\"\n",
    "    print(f\"Filtering dataset: {annotations_file}\")\n",
    "    \n",
    "    annotations = pd.read_csv(annotations_file)\n",
    "    valid_rows = []\n",
    "    au_extractor = FacialActionUnitExtractor()\n",
    "    \n",
    "    for i, row in annotations.iterrows():\n",
    "        # Get file path\n",
    "        if 'video_file' in annotations.columns:\n",
    "            file_path = os.path.join(root_dir, row['video_file'])\n",
    "            is_video = True\n",
    "        else:\n",
    "            file_path = os.path.join(root_dir, row['image_file'])\n",
    "            is_video = False\n",
    "        \n",
    "        if is_video:\n",
    "            # For videos, check a few frames\n",
    "            cap = cv2.VideoCapture(file_path)\n",
    "            if not cap.isOpened():\n",
    "                print(f\"Could not open video file: {file_path}\")\n",
    "                continue\n",
    "            \n",
    "            # Check middle frame\n",
    "            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_count // 2)\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if ret:\n",
    "                frame = cv2.resize(frame, (224, 224))\n",
    "                _, detected = au_extractor.extract_aus(frame, file_path)\n",
    "                \n",
    "                if detected:\n",
    "                    valid_rows.append(row)\n",
    "            \n",
    "            cap.release()\n",
    "        else:\n",
    "            # For images, check the image\n",
    "            img = cv2.imread(file_path)\n",
    "            if img is None:\n",
    "                print(f\"Could not find image: {file_path}\")\n",
    "                continue\n",
    "            \n",
    "            img = cv2.resize(img, (224, 224))\n",
    "            _, detected = au_extractor.extract_aus(img, file_path)\n",
    "            \n",
    "            if detected:\n",
    "                valid_rows.append(row)\n",
    "        \n",
    "        # Report progress\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Processed {i+1}/{len(annotations)}, valid: {len(valid_rows)}\")\n",
    "    \n",
    "    # Create filtered dataset\n",
    "    valid_df = pd.DataFrame(valid_rows)\n",
    "    valid_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Report results\n",
    "    print(f\"Filtered dataset created: {output_file}\")\n",
    "    print(f\"Original dataset: {len(annotations)} samples\")\n",
    "    print(f\"Filtered dataset: {len(valid_df)} samples ({len(valid_df)/len(annotations)*100:.2f}%)\")\n",
    "    \n",
    "    return valid_df\n",
    "\n",
    "# ===== EMOTION RECOGNITION INTEGRATION =====\n",
    "\n",
    "def pd_from_emotions(emotion_probs):\n",
    "    \"\"\"Estimate PD likelihood from emotion probabilities\n",
    "    \n",
    "    Args:\n",
    "        emotion_probs: Array of emotion probabilities [surprise, fear, disgust, happy, sad, angry, neutral]\n",
    "        \n",
    "    Returns:\n",
    "        Estimated PD probability (0-1)\n",
    "    \"\"\"\n",
    "    # PD is characterized by reduced emotional expressivity and increased neutral expression\n",
    "    # This is a simplified heuristic - you should tune these weights based on clinical data\n",
    "    \n",
    "    # Weight for each emotion (higher weight = more important for PD detection)\n",
    "    weights = np.array([0.1, 0.1, 0.1, 0.3, 0.1, 0.1, 0.2])\n",
    "    \n",
    "    # PD indicators (for each emotion: low value = PD indication, high value = healthy indication)\n",
    "    # Flip for neutral emotion where high values indicate PD\n",
    "    indicators = np.array([\n",
    "        1.0,  # surprise - reduced in PD\n",
    "        1.0,  # fear - reduced in PD\n",
    "        1.0,  # disgust - reduced in PD\n",
    "        1.0,  # happy - significantly reduced in PD\n",
    "        0.8,  # sad - slightly reduced in PD\n",
    "        0.8,  # angry - slightly reduced in PD\n",
    "        -1.0  # neutral - increased in PD (note the negative sign)\n",
    "    ])\n",
    "    \n",
    "    # Calculate weighted indicators\n",
    "    weighted_indicators = weights * indicators * emotion_probs\n",
    "    \n",
    "    # Sum and normalize to 0-1 range\n",
    "    pd_indicator = 1.0 - (sum(weighted_indicators) + 0.3) / 0.6\n",
    "    \n",
    "    # Clip to valid probability range\n",
    "    return max(0.0, min(1.0, pd_indicator))\n",
    "\n",
    "# ===== COMBINED FEATURE EXTRACTION =====\n",
    "\n",
    "class CombinedPDDetector:\n",
    "    \"\"\"Class that combines emotion recognition with facial AUs for PD detection\"\"\"\n",
    "    \n",
    "    def __init__(self, emotion_model_path, pd_model_paths=None):\n",
    "        \"\"\"\n",
    "        Initialize the detector with pre-trained models\n",
    "        \n",
    "        Args:\n",
    "            emotion_model_path: Path to your pre-trained emotion recognition model\n",
    "            pd_model_paths: Dictionary of paths to PD-specific models\n",
    "                (e.g., {'rf': 'rf_model.pkl', 'simple': 'simple_pd_model.pth'})\n",
    "        \"\"\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                                  \"mps\" if torch.backends.mps.is_available() else \n",
    "                                  \"cpu\")\n",
    "        \n",
    "        # Load your emotion recognition model\n",
    "        try:\n",
    "            # Placeholder - replace with your actual emotion model loading code\n",
    "            # self.emotion_model = torch.load(emotion_model_path, map_location=self.device)\n",
    "            # self.emotion_model.eval()\n",
    "            self.emotion_model = None  # Placeholder\n",
    "            print(\"Emotion model loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading emotion model: {e}\")\n",
    "            self.emotion_model = None\n",
    "        \n",
    "        # Load PD-specific models if provided\n",
    "        self.pd_models = {}\n",
    "        if pd_model_paths:\n",
    "            try:\n",
    "                # Load Random Forest if available\n",
    "                if 'rf' in pd_model_paths:\n",
    "                    with open(pd_model_paths['rf'], 'rb') as f:\n",
    "                        self.pd_models['rf'] = pickle.load(f)\n",
    "                \n",
    "                # Load Simple Model if available\n",
    "                if 'simple' in pd_model_paths:\n",
    "                    self.pd_models['simple'] = SimplePDModel(input_size=13)\n",
    "                    self.pd_models['simple'].load_state_dict(\n",
    "                        torch.load(pd_model_paths['simple'], map_location=self.device)\n",
    "                    )\n",
    "                    self.pd_models['simple'].to(self.device)\n",
    "                    self.pd_models['simple'].eval()\n",
    "                \n",
    "                # Load Temporal Model if available\n",
    "                if 'temporal' in pd_model_paths:\n",
    "                    self.pd_models['temporal'] = TemporalPDModel(input_size=13, hidden_size=64, num_layers=2)\n",
    "                    self.pd_models['temporal'].load_state_dict(\n",
    "                        torch.load(pd_model_paths['temporal'], map_location=self.device)\n",
    "                    )\n",
    "                    self.pd_models['temporal'].to(self.device)\n",
    "                    self.pd_models['temporal'].eval()\n",
    "                \n",
    "                print(f\"Loaded {len(self.pd_models)} PD models\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading PD models: {e}\")\n",
    "        \n",
    "        # Initialize feature extractors\n",
    "        self.au_extractor = FacialActionUnitExtractor()\n",
    "    \n",
    "    def detect_from_image(self, image_path, use_emotion=True, use_aus=True):\n",
    "        \"\"\"\n",
    "        Perform PD detection on a single image\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to input image\n",
    "            use_emotion: Whether to use emotion recognition features\n",
    "            use_aus: Whether to use facial action unit features\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with detection results\n",
    "        \"\"\"\n",
    "        # Load image\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            return {\"error\": f\"Could not load image: {image_path}\"}\n",
    "        \n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        \n",
    "        # Create results dictionary\n",
    "        results = {\n",
    "            \"image_path\": image_path,\n",
    "            \"pd_probability\": 0.0,\n",
    "            \"emotion_features\": None,\n",
    "            \"au_features\": None,\n",
    "            \"model_predictions\": {}\n",
    "        }\n",
    "        \n",
    "        # Extract emotion features if requested\n",
    "        if use_emotion and self.emotion_model:\n",
    "            try:\n",
    "                # Convert image for emotion model\n",
    "                rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                # This is a placeholder - replace with your actual emotion model preprocessing\n",
    "                # emotion_input = preprocess_for_emotion_model(rgb_img)\n",
    "                # emotion_input = emotion_input.to(self.device)\n",
    "                \n",
    "                # Get emotion probabilities\n",
    "                # with torch.no_grad():\n",
    "                #     emotion_probs = self.emotion_model(emotion_input)\n",
    "                #     emotion_probs = emotion_probs.cpu().numpy()\n",
    "                \n",
    "                # Placeholder emotion probabilities\n",
    "                emotion_probs = np.array([0.1, 0.1, 0.1, 0.3, 0.1, 0.1, 0.2])\n",
    "                \n",
    "                # Calculate PD probability from emotions\n",
    "                pd_from_emotion = pd_from_emotions(emotion_probs)\n",
    "                \n",
    "                results[\"emotion_features\"] = emotion_probs\n",
    "                results[\"model_predictions\"][\"emotion_based\"] = pd_from_emotion\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing emotions: {e}\")\n",
    "        \n",
    "        # Extract AU features if requested\n",
    "        if use_aus:\n",
    "            try:\n",
    "                # Extract AUs\n",
    "                aus, detected = self.au_extractor.extract_aus(img, image_path)\n",
    "                \n",
    "                if not detected:\n",
    "                    results[\"error\"] = \"Face not detected in image\"\n",
    "                    return results\n",
    "                \n",
    "                # Generate sequence with variations\n",
    "                au_sequence = []\n",
    "                for i in range(30):  # 30 frames like in training\n",
    "                    decay_factor = (30 - i) / 30\n",
    "                    variation = np.random.normal(0, 0.02 * decay_factor, size=aus.shape)\n",
    "                    au_sequence.append(aus + variation)\n",
    "                \n",
    "                # Calculate variances\n",
    "                au_variances = np.var(au_sequence, axis=0)\n",
    "                \n",
    "                results[\"au_features\"] = au_variances\n",
    "                \n",
    "                # Make predictions with PD models if available\n",
    "                if self.pd_models:\n",
    "                    # Random Forest prediction\n",
    "                    if 'rf' in self.pd_models:\n",
    "                        rf_pred = self.pd_models['rf'].predict_proba([au_variances])[0, 1]\n",
    "                        results[\"model_predictions\"][\"rf\"] = rf_pred\n",
    "                    \n",
    "                    # Simple Model prediction\n",
    "                    if 'simple' in self.pd_models:\n",
    "                        with torch.no_grad():\n",
    "                            simple_input = torch.FloatTensor(au_variances).to(self.device)\n",
    "                            simple_pred = self.pd_models['simple'](simple_input).item()\n",
    "                            results[\"model_predictions\"][\"simple\"] = simple_pred\n",
    "                    \n",
    "                    # Temporal Model prediction\n",
    "                    if 'temporal' in self.pd_models:\n",
    "                        with torch.no_grad():\n",
    "                            sequence_np = np.array(au_sequence)\n",
    "                            temporal_input = torch.FloatTensor(sequence_np).unsqueeze(0).to(self.device)\n",
    "                            temporal_pred = self.pd_models['temporal'](temporal_input).item()\n",
    "                            results[\"model_predictions\"][\"temporal\"] = temporal_pred\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing AUs: {e}\")\n",
    "        \n",
    "        # Calculate final probability (ensemble)\n",
    "        if results[\"model_predictions\"]:\n",
    "            # Average all available predictions\n",
    "            pd_probability = sum(results[\"model_predictions\"].values()) / len(results[\"model_predictions\"])\n",
    "            results[\"pd_probability\"] = pd_probability\n",
    "            \n",
    "            # Add likelihood category\n",
    "            if pd_probability > 0.7:\n",
    "                results[\"pd_likelihood\"] = \"High\"\n",
    "            elif pd_probability > 0.3:\n",
    "                results[\"pd_likelihood\"] = \"Medium\"\n",
    "            else:\n",
    "                results[\"pd_likelihood\"] = \"Low\"\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def detect_from_video(self, video_path, use_emotion=True, use_aus=True):\n",
    "        \"\"\"\n",
    "        Perform PD detection on a video\n",
    "        \n",
    "        Args:\n",
    "            video_path: Path to input video\n",
    "            use_emotion: Whether to use emotion recognition features\n",
    "            use_aus: Whether to use facial action unit features\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with detection results\n",
    "        \"\"\"\n",
    "        # Check if video exists\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            return {\"error\": f\"Could not open video: {video_path}\"}\n",
    "        \n",
    "        # Get video properties\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        \n",
    "        # Select frames to process\n",
    "        if frame_count > 30:\n",
    "            indices = np.linspace(0, frame_count-1, 30, dtype=int)\n",
    "        else:\n",
    "            indices = range(frame_count)\n",
    "        \n",
    "        # Create results dictionary\n",
    "        results = {\n",
    "            \"video_path\": video_path,\n",
    "            \"pd_probability\": 0.0,\n",
    "            \"frames_processed\": 0,\n",
    "            \"face_detection_rate\": 0.0,\n",
    "            \"emotion_features\": None,\n",
    "            \"au_features\": None,\n",
    "            \"model_predictions\": {}\n",
    "        }\n",
    "        \n",
    "        # Process frames\n",
    "        emotion_probs_list = []\n",
    "        au_sequence = []\n",
    "        face_detection_count = 0\n",
    "        \n",
    "        for i in indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if not ret:\n",
    "                continue\n",
    "            \n",
    "            # Resize frame\n",
    "            frame = cv2.resize(frame, (224, 224))\n",
    "            \n",
    "            # Extract emotions if requested\n",
    "            if use_emotion and self.emotion_model:\n",
    "                try:\n",
    "                    # Convert frame for emotion model\n",
    "                    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    # This is a placeholder - replace with your actual emotion model preprocessing\n",
    "                    # emotion_input = preprocess_for_emotion_model(rgb_frame)\n",
    "                    # emotion_input = emotion_input.to(self.device)\n",
    "                    \n",
    "                    # Get emotion probabilities\n",
    "                    # with torch.no_grad():\n",
    "                    #     emotion_probs = self.emotion_model(emotion_input)\n",
    "                    #     emotion_probs = emotion_probs.cpu().numpy()\n",
    "                    \n",
    "                    # Placeholder emotion probabilities\n",
    "                    emotion_probs = np.array([0.1, 0.1, 0.1, 0.3, 0.1, 0.1, 0.2])\n",
    "                    emotion_probs_list.append(emotion_probs)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing emotions for frame {i}: {e}\")\n",
    "            \n",
    "            # Extract AUs if requested\n",
    "            if use_aus:\n",
    "                try:\n",
    "                    # Extract AUs\n",
    "                    aus, detected = self.au_extractor.extract_aus(frame)\n",
    "                    \n",
    "                    if detected:\n",
    "                        face_detection_count += 1\n",
    "                    \n",
    "                    # Add to sequence\n",
    "                    au_sequence.append(aus)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing AUs for frame {i}: {e}\")\n",
    "                    au_sequence.append(np.zeros(13))\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        # Update results\n",
    "        results[\"frames_processed\"] = len(indices)\n",
    "        results[\"face_detection_rate\"] = face_detection_count / len(indices) if indices else 0\n",
    "        \n",
    "        # Process emotion features\n",
    "        if emotion_probs_list:\n",
    "            # Average emotion probabilities across frames\n",
    "            avg_emotion_probs = np.mean(emotion_probs_list, axis=0)\n",
    "            results[\"emotion_features\"] = avg_emotion_probs\n",
    "            \n",
    "            # Calculate PD probability from emotions\n",
    "            pd_from_emotion = pd_from_emotions(avg_emotion_probs)\n",
    "            results[\"model_predictions\"][\"emotion_based\"] = pd_from_emotion\n",
    "        \n",
    "        # Process AU features\n",
    "        if au_sequence:\n",
    "            # Calculate variances\n",
    "            au_variances = np.var(au_sequence, axis=0)\n",
    "            results[\"au_features\"] = au_variances\n",
    "            \n",
    "            # Make predictions with PD models if available\n",
    "            if self.pd_models:\n",
    "                # Random Forest prediction\n",
    "                if 'rf' in self.pd_models:\n",
    "                    rf_pred = self.pd_models['rf'].predict_proba([au_variances])[0, 1]\n",
    "                    results[\"model_predictions\"][\"rf\"] = rf_pred\n",
    "                \n",
    "                # Simple Model prediction\n",
    "                if 'simple' in self.pd_models:\n",
    "                    with torch.no_grad():\n",
    "                        simple_input = torch.FloatTensor(au_variances).to(self.device)\n",
    "                        simple_pred = self.pd_models['simple'](simple_input).item()\n",
    "                        results[\"model_predictions\"][\"simple\"] = simple_pred\n",
    "                \n",
    "                # Temporal Model prediction\n",
    "                if 'temporal' in self.pd_models:\n",
    "                    with torch.no_grad():\n",
    "                        sequence_np = np.array(au_sequence)\n",
    "                        temporal_input = torch.FloatTensor(sequence_np).unsqueeze(0).to(self.device)\n",
    "                        temporal_pred = self.pd_models['temporal'](temporal_input).item()\n",
    "                        results[\"model_predictions\"][\"temporal\"] = temporal_pred\n",
    "        \n",
    "        # Calculate final probability (ensemble)\n",
    "        if results[\"model_predictions\"]:\n",
    "            # Average all available predictions\n",
    "            pd_probability = sum(results[\"model_predictions\"].values()) / len(results[\"model_predictions\"])\n",
    "            results[\"pd_probability\"] = pd_probability\n",
    "            \n",
    "            # Add likelihood category\n",
    "            if pd_probability > 0.7:\n",
    "                results[\"pd_likelihood\"] = \"High\"\n",
    "            elif pd_probability > 0.3:\n",
    "                results[\"pd_likelihood\"] = \"Medium\"\n",
    "            else:\n",
    "                results[\"pd_likelihood\"] = \"Low\"\n",
    "        \n",
    "        return results\n",
    "\n",
    "# ===== MAIN FUNCTION WITH DETECTION =====\n",
    "\n",
    "def main():\n",
    "    # Set paths\n",
    "    data_dir = '/path/to/your/data'\n",
    "    emotion_model_path = '/path/to/your/emotion_model.pth'\n",
    "    \n",
    "    # Filter original dataset to only include images with detectable faces\n",
    "    # This is crucial for preventing errors later\n",
    "    filter_dataset(\n",
    "        root_dir=data_dir,\n",
    "        annotations_file=os.path.join(data_dir, 'pd_annotations_balanced.csv'),\n",
    "        output_file=os.path.join(data_dir, 'pd_annotations_filtered.csv')\n",
    "    )\n",
    "    \n",
    "    # Split into train/val sets (80/20 split)\n",
    "    annotations = pd.read_csv(os.path.join(data_dir, 'pd_annotations_filtered.csv'))\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_df, val_df = train_test_split(annotations, test_size=0.2, stratify=annotations['has_pd'], random_state=42)\n",
    "    \n",
    "    # Save train/val splits\n",
    "    train_df.to_csv(os.path.join(data_dir, 'pd_train_filtered.csv'), index=False)\n",
    "    val_df.to_csv(os.path.join(data_dir, 'pd_val_filtered.csv'), index=False)\n",
    "    \n",
    "    # Create datasets with feature inspection enabled\n",
    "    train_dataset = PDDataset(\n",
    "        root_dir=data_dir,\n",
    "        annotations_file=os.path.join(data_dir, 'pd_train_filtered.csv'),\n",
    "        inspect_features=True,\n",
    "        balance_classes=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = PDDataset(\n",
    "        root_dir=data_dir,\n",
    "        annotations_file=os.path.join(data_dir, 'pd_val_filtered.csv'),\n",
    "        inspect_features=True,\n",
    "        balance_classes=True\n",
    "    )\n",
    "    \n",
    "    # Validate features - see if they can distinguish PD from non-PD\n",
    "    feature_stats = train_dataset.get_feature_statistics()\n",
    "    \n",
    "    # Print feature differences\n",
    "    print(\"\\nFeature Differences (PD vs non-PD):\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, name in enumerate(feature_stats[\"feature_names\"]):\n",
    "        diff = feature_stats[\"differences\"][i]\n",
    "        t_stat = feature_stats[\"t_statistics\"][i]\n",
    "        pd_val = feature_stats[\"pd_means\"][i]\n",
    "        non_pd_val = feature_stats[\"non_pd_means\"][i]\n",
    "        \n",
    "        significance = \"\"\n",
    "        if abs(t_stat) > 2.58:\n",
    "            significance = \"*** (p<0.01)\"\n",
    "        elif abs(t_stat) > 1.96:\n",
    "            significance = \"** (p<0.05)\"\n",
    "        elif abs(t_stat) > 1.65:\n",
    "            significance = \"* (p<0.1)\"\n",
    "        \n",
    "        print(f\"{name}: PD={pd_val:.5f}, Non-PD={non_pd_val:.5f}, Diff={diff:.5f}, t={t_stat:.2f} {significance}\")\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Define models\n",
    "    simple_model = SimplePDModel(input_size=13).to(device)\n",
    "    temporal_model = TemporalPDModel(input_size=13, hidden_size=64, num_layers=2).to(device)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = F.binary_cross_entropy\n",
    "    \n",
    "    # Train simple model\n",
    "    simple_optimizer = optim.AdamW(simple_model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    simple_scheduler = optim.lr_scheduler.ReduceLROnPlateau(simple_optimizer, 'min', patience=3, factor=0.5)\n",
    "    \n",
    "    # Train temporal model\n",
    "    temporal_optimizer = optim.AdamW(temporal_model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    temporal_scheduler = optim.lr_scheduler.ReduceLROnPlateau(temporal_optimizer, 'min', patience=3, factor=0.5)\n",
    "    \n",
    "    # Train models (simplified training loop)\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        # Train simple model\n",
    "        simple_model.train()\n",
    "        for _, variances, labels in train_loader:\n",
    "            variances = variances.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            simple_optimizer.zero_grad()\n",
    "            outputs = simple_model(variances)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            simple_optimizer.step()\n",
    "        \n",
    "        # Train temporal model\n",
    "        temporal_model.train()\n",
    "        for sequences, _, labels in train_loader:\n",
    "            sequences = sequences.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            temporal_optimizer.zero_grad()\n",
    "            outputs = temporal_model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            temporal_optimizer.step()\n",
    "    \n",
    "    # Save models\n",
    "    torch.save(simple_model.state_dict(), 'simple_pd_model.pth')\n",
    "    torch.save(temporal_model.state_dict(), 'temporal_pd_model.pth')\n",
    "    \n",
    "    print(\"Training complete. Models saved.\")\n",
    "    \n",
    "    # Create combined detector\n",
    "    detector = CombinedPDDetector(\n",
    "        emotion_model_path=emotion_model_path,\n",
    "        pd_model_paths={\n",
    "            'simple': 'simple_pd_model.pth',\n",
    "            'temporal': 'temporal_pd_model.pth'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Test on an image\n",
    "    test_image = os.path.join(data_dir, 'test_image.jpg')\n",
    "    if os.path.exists(test_image):\n",
    "        results = detector.detect_from_image(test_image)\n",
    "        print(\"\\nPD Detection Results:\")\n",
    "        print(f\"PD Probability: {results['pd_probability']:.4f}\")\n",
    "        if 'pd_likelihood' in results:\n",
    "            print(f\"PD Likelihood: {results['pd_likelihood']}\")\n",
    "        print(\"Model Predictions:\")\n",
    "        for model_name, pred in results.get(\"model_predictions\", {}).items():\n",
    "            print(f\"  {model_name}: {pred:.4f}\")\n",
    "    \n",
    "    # Create live video detection function\n",
    "    def run_live_detection():\n",
    "        \"\"\"Run live PD detection using webcam\"\"\"\n",
    "        # Initialize combined detector\n",
    "        detector = CombinedPDDetector(\n",
    "            emotion_model_path=emotion_model_path,\n",
    "            pd_model_paths={\n",
    "                'simple': 'simple_pd_model.pth',\n",
    "                'temporal': 'temporal_pd_model.pth'\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Open webcam\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        if not cap.isOpened():\n",
    "            print(\"Could not open webcam\")\n",
    "            return\n",
    "        \n",
    "        # For temporal features\n",
    "        frame_buffer = []\n",
    "        buffer_size = 30  # Same as in training\n",
    "        \n",
    "        print(\"PD Detection started. Press 'q' to quit.\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        frame_count = 0\n",
    "        \n",
    "        while True:\n",
    "            # Capture frame\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Failed to capture frame\")\n",
    "                break\n",
    "                \n",
    "            # Resize for processing\n",
    "            process_frame = cv2.resize(frame, (224, 224))\n",
    "            \n",
    "            # Extract AUs\n",
    "            aus, detected = detector.au_extractor.extract_aus(process_frame)\n",
    "            \n",
    "            # Add to buffer\n",
    "            frame_buffer.append(aus)\n",
    "            if len(frame_buffer) > buffer_size:\n",
    "                frame_buffer.pop(0)\n",
    "                \n",
    "            # If buffer is full and face detected, make prediction\n",
    "            if len(frame_buffer) == buffer_size and detected:\n",
    "                # Calculate variances\n",
    "                au_variances = np.var(frame_buffer, axis=0)\n",
    "                \n",
    "                # Make predictions\n",
    "                predictions = {}\n",
    "                \n",
    "                # 1. Random Forest (if available)\n",
    "                if 'rf' in detector.pd_models:\n",
    "                    rf_pred = detector.pd_models['rf'].predict_proba([au_variances])[0, 1]\n",
    "                    predictions['rf'] = rf_pred\n",
    "                \n",
    "                # 2. Simple Model\n",
    "                if 'simple' in detector.pd_models:\n",
    "                    with torch.no_grad():\n",
    "                        simple_input = torch.FloatTensor(au_variances).to(device)\n",
    "                        simple_pred = detector.pd_models['simple'](simple_input).item()\n",
    "                        predictions['simple'] = simple_pred\n",
    "                \n",
    "                # 3. Temporal Model\n",
    "                if 'temporal' in detector.pd_models:\n",
    "                    with torch.no_grad():\n",
    "                        sequence_np = np.array(frame_buffer)\n",
    "                        temporal_input = torch.FloatTensor(sequence_np).unsqueeze(0).to(device)\n",
    "                        temporal_pred = detector.pd_models['temporal'](temporal_input).item()\n",
    "                        predictions['temporal'] = temporal_pred\n",
    "                \n",
    "                # Ensemble prediction\n",
    "                if predictions:\n",
    "                    ensemble_pred = sum(predictions.values()) / len(predictions)\n",
    "                    \n",
    "                    # Determine likelihood category\n",
    "                    if ensemble_pred > 0.7:\n",
    "                        likelihood = \"High\"\n",
    "                        color = (0, 0, 255)  # Red\n",
    "                    elif ensemble_pred > 0.3:\n",
    "                        likelihood = \"Medium\"\n",
    "                        color = (0, 165, 255)  # Orange\n",
    "                    else:\n",
    "                        likelihood = \"Low\"\n",
    "                        color = (0, 255, 0)  # Green\n",
    "                    \n",
    "                    # Display prediction on frame\n",
    "                    cv2.putText(frame, f\"PD Probability: {ensemble_pred:.2f} ({likelihood})\", \n",
    "                                (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "                    \n",
    "                    # Display key features\n",
    "                    features = {\n",
    "                        'Cheek Raiser': au_variances[0],\n",
    "                        'Lip Corner Puller': au_variances[1],\n",
    "                        'Brow Lowerer': au_variances[2],\n",
    "                        'Smile Asymmetry': au_variances[9],\n",
    "                        'Facial Mobility': au_variances[11]\n",
    "                    }\n",
    "                    \n",
    "                    y_pos = 60\n",
    "                    for name, value in features.items():\n",
    "                        # Normalize feature value for display\n",
    "                        norm_value = min(1.0, max(0.0, value / 0.05))  # Adjust denominator as needed\n",
    "                        feature_text = f\"{name}: {value:.4f}\"\n",
    "                        cv2.putText(frame, feature_text, (10, y_pos), \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                        \n",
    "                        # Draw bar representation\n",
    "                        bar_length = int(150 * norm_value)\n",
    "                        cv2.rectangle(frame, (200, y_pos-15), (200+bar_length, y_pos-5), color, -1)\n",
    "                        \n",
    "                        y_pos += 25\n",
    "                    \n",
    "                    # Calculate FPS\n",
    "                    frame_count += 1\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    if elapsed_time >= 1.0:\n",
    "                        fps = frame_count / elapsed_time\n",
    "                        frame_count = 0\n",
    "                        start_time = time.time()\n",
    "                        cv2.putText(frame, f\"FPS: {fps:.1f}\", (frame.shape[1]-120, 30), \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "            else:\n",
    "                # Display status when face not detected or buffer not full\n",
    "                if not detected:\n",
    "                    cv2.putText(frame, \"Face not detected\", (10, 30), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                elif len(frame_buffer) < buffer_size:\n",
    "                    cv2.putText(frame, f\"Buffering: {len(frame_buffer)}/{buffer_size}\", (10, 30), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "            \n",
    "            # Show frame\n",
    "            cv2.imshow('Parkinson\\'s Disease Detection', frame)\n",
    "            \n",
    "            # Check for exit\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        \n",
    "        # Release resources\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    # Add the function to be callable\n",
    "    print(\"\\nTo run live detection, call run_live_detection()\")\n",
    "    \n",
    "    # Return the live detection function\n",
    "    return run_live_detection\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_live_detection = main()\n",
    "    # Uncomment to run live detection immediately\n",
    "    # run_live_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second cell - Run the live detector\n",
    "%run pd_live_detector.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
